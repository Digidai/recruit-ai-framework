#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆå’Œæ›´æ–° sitemap.xml
æ ¹æ® tarf.json ä¸­çš„èµ„æºè‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„ sitemap
"""

import json
import os
from datetime import datetime
from pathlib import Path
import xml.etree.ElementTree as ET
from xml.dom import minidom

# é…ç½®
BASE_URL = "https://digidai.github.io/recruit-ai-framework"
# è·å–é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
DOCS_DIR = PROJECT_ROOT / "docs"
TARF_FILE = DOCS_DIR / "tarf.json"
SITEMAP_FILE = DOCS_DIR / "sitemap.xml"
SITEMAP_XSL = "https://digidai.github.io/recruit-ai-framework/sitemap.xsl"


def load_resources():
    """åŠ è½½ tarf.json ä¸­çš„æ‰€æœ‰èµ„æº"""
    try:
        with open(TARF_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error loading tarf.json: {e}")
        return {}


def extract_all_urls(data, path=''):
    """é€’å½’æå–æ‰€æœ‰ URL èµ„æº"""
    urls = []

    # æ·»åŠ æ ¹é¡µé¢
    urls.append({
        'name': data.get('name', ''),
        'url': f"{BASE_URL}/",
        'priority': 1.0
    })

    def traverse(node, depth=0):
        """é€’å½’éå†èŠ‚ç‚¹"""
        if node.get('type') == 'url':
            # è¿™æ˜¯ä¸€ä¸ª URL èµ„æº
            url = node.get('url', '')
            if url and not url.startswith(BASE_URL):
                # å¦‚æœæ˜¯å¤–éƒ¨é“¾æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå†…éƒ¨èµ„æºé¡µé¢
                resource_id = f"res-{len(urls)}"
                urls.append({
                    'name': node.get('name', ''),
                    'name_en': node.get('name_en', ''),
                    'url': f"{BASE_URL}/?resource={resource_id}",
                    'priority': max(0.6, 1.0 - depth * 0.1)
                })

        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.get('children', []):
            traverse(child, depth + 1)

    traverse(data)
    return urls


def create_url_element(url, lastmod, changefreq, priority):
    """åˆ›å»ºä¸€ä¸ª URL å…ƒç´ """
    url_elem = ET.Element('url')

    loc = ET.SubElement(url_elem, 'loc')
    loc.text = url

    lastmod_elem = ET.SubElement(url_elem, 'lastmod')
    lastmod_elem.text = lastmod

    changefreq_elem = ET.SubElement(url_elem, 'changefreq')
    changefreq_elem.text = changefreq

    priority_elem = ET.SubElement(url_elem, 'priority')
    priority_elem.text = str(priority)

    return url_elem


def generate_sitemap():
    """ç”Ÿæˆå®Œæ•´çš„ sitemap"""
    # åˆ›å»ºæ ¹å…ƒç´ 
    urlset = ET.Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    urlset.set('xmlns:xsi', 'http://www.w3.org/2001/XMLSchema-instance')
    urlset.set('xsi:schemaLocation',
                'http://www.sitemaps.org/schemas/sitemap/0.9 '
                'http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd')

    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().strftime('%Y-%m-%d')

    # 1. æ·»åŠ ä¸»é¡µï¼ˆå·²ç»åœ¨ extract_all_urls ä¸­æ·»åŠ ï¼Œä½†ä¸ºäº†ç¡®ä¿ä¼˜å…ˆçº§ï¼Œå†æ¬¡æ·»åŠ ï¼‰
    urlset.append(create_url_element(
        f"{BASE_URL}/",
        today,
        'weekly',
        1.0
    ))

    urlset.append(create_url_element(
        f"{BASE_URL}/index.html",
        today,
        'weekly',
        1.0
    ))

    # 2. åŠ è½½å¹¶æ·»åŠ æ‰€æœ‰èµ„æºé¡µé¢
    data = load_resources()
    urls = extract_all_urls(data)

    # è¿‡æ»¤æ‰æ ¹ä¸»é¡µï¼ˆå› ä¸ºå·²ç»æ·»åŠ è¿‡äº†ï¼‰
    urls = [u for u in urls if not u['url'] == f"{BASE_URL}/"]

    print(f"Found {len(urls)} resource URLs")

    for url_data in urls:
        urlset.append(create_url_element(
            url_data['url'],
            today,
            'monthly',
            url_data.get('priority', 0.8)
        ))

    # 3. æ·»åŠ å…¶ä»–é™æ€é¡µé¢
    static_pages = [
        ('404.html', 0.3, 'monthly'),
        ('manifest.json', 0.3, 'monthly'),
    ]

    for page, priority, changefreq in static_pages:
        urlset.append(create_url_element(
            f"{BASE_URL}/{page}",
            today,
            changefreq,
            priority
        ))

    # ç”Ÿæˆ XML å­—ç¬¦ä¸²
    xml_str = minidom.parseString(ET.tostring(urlset)).toprettyxml(indent="  ")

    # æ·»åŠ  XSL æ ·å¼è¡¨
    xml_lines = xml_str.split('\n')
    xml_lines.insert(1, f'<?xml-stylesheet type="text/xsl" href="{SITEMAP_XSL}"?>')
    xml_str = '\n'.join(xml_lines)

    return xml_str


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting sitemap generation...")

    # ç”Ÿæˆ sitemap
    sitemap_content = generate_sitemap()

    # å†™å…¥æ–‡ä»¶
    with open(SITEMAP_FILE, 'w', encoding='utf-8') as f:
        f.write(sitemap_content)

    print(f"âœ… Sitemap generated successfully: {SITEMAP_FILE}")
    print(f"ğŸ“Š Total URLs in sitemap: {sitemap_content.count('<url>')}")


if __name__ == "__main__":
    main()
